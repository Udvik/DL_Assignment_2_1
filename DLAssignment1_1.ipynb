{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import os\n"
      ],
      "metadata": {
        "id": "rQClCTHme2Pm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data (Latin to Devanagari character sequence)\n",
        "data_pairs = [\n",
        "    (\"namaste\", \"नमस्ते\"),\n",
        "    (\"shiva\", \"शिव\"),\n",
        "    (\"ganesha\", \"गणेश\"),\n",
        "    (\"ram\", \"राम\"),\n",
        "    (\"krishna\", \"कृष्ण\"),\n",
        "    (\"sita\", \"सीता\")\n",
        "]\n",
        "\n",
        "# Extract character sets\n",
        "input_chars = sorted(list(set(\"\".join(pair[0] for pair in data_pairs))))\n",
        "output_chars = sorted(list(set(\"\".join(pair[1] for pair in data_pairs))))\n",
        "\n",
        "input_char2idx = {ch: idx + 1 for idx, ch in enumerate(input_chars)}  # index 0 is for PAD\n",
        "input_char2idx[\"<PAD>\"] = 0\n",
        "output_char2idx = {ch: idx + 1 for idx, ch in enumerate(output_chars)}\n",
        "output_char2idx[\"<PAD>\"] = 0\n",
        "output_char2idx[\"<SOS>\"] = len(output_char2idx)\n",
        "output_char2idx[\"<EOS>\"] = len(output_char2idx)\n",
        "\n",
        "idx2output_char = {idx: ch for ch, idx in output_char2idx.items()}\n",
        "\n",
        "# Parameters\n",
        "MAX_LENGTH = max(max(len(x), len(y)) for x, y in data_pairs) + 2  # +2 for <SOS> and <EOS>\n"
      ],
      "metadata": {
        "id": "aZsa6KtniQOS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharSeqDataset(Dataset):\n",
        "    def __init__(self, data_pairs):\n",
        "        self.data = data_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        src_seq = [input_char2idx[ch] for ch in src]\n",
        "        tgt_seq = [output_char2idx[\"<SOS>\"]] + [output_char2idx[ch] for ch in tgt] + [output_char2idx[\"<EOS>\"]]\n",
        "\n",
        "        src_seq += [input_char2idx[\"<PAD>\"]] * (MAX_LENGTH - len(src_seq))\n",
        "        tgt_seq += [output_char2idx[\"<PAD>\"]] * (MAX_LENGTH - len(tgt_seq))\n",
        "\n",
        "        return torch.tensor(src_seq), torch.tensor(tgt_seq[:-1]), torch.tensor(tgt_seq[1:])  # input, decoder_input, target\n"
      ],
      "metadata": {
        "id": "LBaRLN3UiR7S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, input_vocab_size, target_vocab_size, embedding_dim, hidden_dim,\n",
        "                 num_layers=1, cell_type='lstm'):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "\n",
        "        self.cell_type = cell_type.lower()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.encoder_embedding = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.decoder_embedding = nn.Embedding(target_vocab_size, embedding_dim, padding_idx=0)\n",
        "\n",
        "        rnn = {'rnn': nn.RNN, 'lstm': nn.LSTM, 'gru': nn.GRU}[self.cell_type]\n",
        "\n",
        "        self.encoder = rnn(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.decoder = rnn(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "\n",
        "        self.output_fc = nn.Linear(hidden_dim, target_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        # Embeddings\n",
        "        src_embed = self.encoder_embedding(src)\n",
        "        tgt_embed = self.decoder_embedding(tgt)\n",
        "\n",
        "        # Encoder\n",
        "        _, hidden = self.encoder(src_embed)\n",
        "\n",
        "        # Decoder\n",
        "        output, _ = self.decoder(tgt_embed, hidden)\n",
        "\n",
        "        return self.output_fc(output)\n"
      ],
      "metadata": {
        "id": "QjlhfYqJiTjJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_seq2seq(model, dataloader, criterion, optimizer, device, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt_input, tgt_output in dataloader:\n",
        "            src, tgt_input, tgt_output = src.to(device), tgt_input.to(device), tgt_output.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(src, tgt_input)  # (B, T, V)\n",
        "            loss = criterion(outputs.view(-1, outputs.size(-1)), tgt_output.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "kwX_iI9tiU0h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_seq2seq(model, src_seq, device, max_len=MAX_LENGTH):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert input characters to indices\n",
        "    src_indices = [input_char2idx[ch] for ch in src_seq]\n",
        "    src_indices += [input_char2idx[\"<PAD>\"]] * (MAX_LENGTH - len(src_indices))\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)  # (1, T)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Encoder forward\n",
        "        encoder_embedded = model.encoder_embedding(src_tensor)  # (1, T, embed_dim)\n",
        "        _, hidden = model.encoder(encoder_embedded)\n",
        "\n",
        "        # Start decoder with <SOS>\n",
        "        decoder_input = torch.tensor([[output_char2idx[\"<SOS>\"]]], dtype=torch.long).to(device)  # (1, 1)\n",
        "        decoded_output = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            decoder_embedded = model.decoder_embedding(decoder_input)  # (1, 1, embed_dim)\n",
        "            output, hidden = model.decoder(decoder_embedded, hidden)   # (1, 1, hidden_dim)\n",
        "            logits = model.output_fc(output[:, 0, :])                   # (1, vocab_size)\n",
        "            predicted_id = logits.argmax(1).item()\n",
        "\n",
        "            if idx2output_char[predicted_id] == \"<EOS>\":\n",
        "                break\n",
        "\n",
        "            decoded_output.append(idx2output_char[predicted_id])\n",
        "            decoder_input = torch.tensor([[predicted_id]], dtype=torch.long).to(device)  # (1, 1)\n",
        "\n",
        "    return ''.join(decoded_output)\n"
      ],
      "metadata": {
        "id": "BYl7OA0diZlm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 1\n",
        "cell_type = 'lstm'\n",
        "\n",
        "# Vocabulary sizes\n",
        "input_vocab_size = len(input_char2idx)\n",
        "target_vocab_size = len(output_char2idx)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = CharSeqDataset(data_pairs)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Model\n",
        "model = Seq2SeqModel(input_vocab_size, target_vocab_size, embedding_dim, hidden_dim,\n",
        "                     num_layers=num_layers, cell_type=cell_type).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train\n",
        "train_seq2seq(model, dataloader, criterion, optimizer, device, epochs=30)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for x, y in data_pairs:\n",
        "    prediction = predict_seq2seq(model, x, device)\n",
        "    print(f\"{x} -> {prediction}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQaSVXMlic8o",
        "outputId": "931a76db-f769-4916-f59e-4dc1f0aaa394"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.9893\n",
            "Epoch 2, Loss: 2.9047\n",
            "Epoch 3, Loss: 2.8330\n",
            "Epoch 4, Loss: 2.7629\n",
            "Epoch 5, Loss: 2.6846\n",
            "Epoch 6, Loss: 2.5967\n",
            "Epoch 7, Loss: 2.4911\n",
            "Epoch 8, Loss: 2.3620\n",
            "Epoch 9, Loss: 2.2039\n",
            "Epoch 10, Loss: 2.0232\n",
            "Epoch 11, Loss: 1.8540\n",
            "Epoch 12, Loss: 1.6640\n",
            "Epoch 13, Loss: 1.4942\n",
            "Epoch 14, Loss: 1.3076\n",
            "Epoch 15, Loss: 1.1343\n",
            "Epoch 16, Loss: 0.9805\n",
            "Epoch 17, Loss: 0.8342\n",
            "Epoch 18, Loss: 0.7057\n",
            "Epoch 19, Loss: 0.5953\n",
            "Epoch 20, Loss: 0.5129\n",
            "Epoch 21, Loss: 0.4300\n",
            "Epoch 22, Loss: 0.3679\n",
            "Epoch 23, Loss: 0.3119\n",
            "Epoch 24, Loss: 0.2690\n",
            "Epoch 25, Loss: 0.2337\n",
            "Epoch 26, Loss: 0.1991\n",
            "Epoch 27, Loss: 0.1798\n",
            "Epoch 28, Loss: 0.1579\n",
            "Epoch 29, Loss: 0.1426\n",
            "Epoch 30, Loss: 0.1282\n",
            "\n",
            "Sample Predictions:\n",
            "namaste -> नमस्ते\n",
            "shiva -> शिव\n",
            "ganesha -> गणेश\n",
            "ram -> राम\n",
            "krishna -> कृष्ण\n",
            "sita -> सीता\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cy9x3v6liiTl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}