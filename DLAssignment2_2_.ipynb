{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO93usq8AZWrkd4l9ksy9YW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXyD7Lk09s20",
        "outputId": "a6ca0abf-9a23-4b02-ec7d-a305f3abe845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/poetry\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/poetry\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Hugging Face Transformers"
      ],
      "metadata": {
        "id": "Wq5ktV9OMcpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovDf8SN1AyYa",
        "outputId": "a8ae659e-6343-4577-a312-6c33cae9e70d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "M7d8rbSkMmD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel, GPT2Tokenizer,\n",
        "    TextDataset, DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments, pipeline, set_seed\n",
        ")\n",
        "import os"
      ],
      "metadata": {
        "id": "MVjqQpGlMeoj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/poetry\"\n",
        "output_file = \"poems.txt\"\n",
        "\n",
        "# Combine all text files into one\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(dataset_path, filename)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as infile:\n",
        "                content = infile.read().strip()\n",
        "                outfile.write(content + \"\\n\\n\")\n",
        "\n",
        "print(f\"✅ Combined all artist lyrics into: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XML1LbleNQ2V",
        "outputId": "d9656754-6c54-44ca-be9b-7e1a6adffec7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined all artist lyrics into: poems.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/poetry\"\n",
        "print(\"Files in dataset folder:\")\n",
        "print(os.listdir(dataset_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOcgFih-OE38",
        "outputId": "0a0cc7bd-316d-4eef-c6b6-34a96bd44fe6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in dataset folder:\n",
            "['Kanye_West.txt', 'johnny-cash.txt', 'kanye-west.txt', 'bruno-mars.txt', 'dickinson.txt', 'amy-winehouse.txt', 'blink-182.txt', 'paul-simon.txt', 'patti-smith.txt', 'bieber.txt', 'disney.txt', 'jimi-hendrix.txt', 'lin-manuel-miranda.txt', 'adele.txt', 'dj-khaled.txt', 'beatles.txt', 'r-kelly.txt', 'lady-gaga.txt', 'radiohead.txt', 'britney-spears.txt', 'alicia-keys.txt', 'rihanna.txt', 'joni-mitchell.txt', 'dolly-parton.txt', 'drake.txt', 'Lil_Wayne.txt', 'notorious_big.txt', 'eminem.txt', 'janisjoplin.txt', 'prince.txt', 'bruce-springsteen.txt', 'bob-dylan.txt', 'notorious-big.txt', 'lil-wayne.txt', 'dr-seuss.txt', 'nicki-minaj.txt', 'bob-marley.txt', 'al-green.txt', 'nickelback.txt', 'michael-jackson.txt', 'lorde.txt', 'kanye.txt', 'leonard-cohen.txt', 'ludacris.txt', 'bjork.txt', 'nursery_rhymes.txt', 'nirvana.txt', 'cake.txt', 'missy-elliott.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔹 Step 5: Load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Required by Trainer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "dhSalWx4OQ1k"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔹 Step 6: Dataset creation\n",
        "def load_dataset(file_path, tokenizer, block_size=64):\n",
        "    from transformers import TextDataset\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "def get_data_collator(tokenizer):\n",
        "    return DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "train_dataset = load_dataset(\"poems.txt\", tokenizer, block_size=64)\n",
        "data_collator = get_data_collator(tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRKMs0SAOdDx",
        "outputId": "d5abc1aa-8db3-4aa8-f836-d192cbf555a6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 🔹 Step 7: Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-poetry\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=6000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=500,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
      ],
      "metadata": {
        "id": "6u9ZS2xcOrO5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9n3g91GDO6rH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}